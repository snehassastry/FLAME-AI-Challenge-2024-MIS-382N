{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "661b16a636d44d2eb0b2b47526e3fe8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3c1d065598042279a9d69405306395e",
              "IPY_MODEL_9e43440db1d1424897cf2b9d19bb2b2a",
              "IPY_MODEL_1d73bc4266884106b6b1de90da88c020"
            ],
            "layout": "IPY_MODEL_84dc00d1172b4b86a09f5132c3fd309e"
          }
        },
        "f3c1d065598042279a9d69405306395e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91903ffbce244a86b803464d154c6c31",
            "placeholder": "​",
            "style": "IPY_MODEL_d54c7be10e874b2b8d0c39a4a44b5253",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "9e43440db1d1424897cf2b9d19bb2b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a981e25ac5d44bd08eab7868585ed1bb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a738e84dce684a01a1bacd4d3574ab34",
            "value": 0
          }
        },
        "1d73bc4266884106b6b1de90da88c020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_996a576e712c457db93c99bf69155f48",
            "placeholder": "​",
            "style": "IPY_MODEL_a02c170a70e84f718046b68c77982653",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "84dc00d1172b4b86a09f5132c3fd309e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "91903ffbce244a86b803464d154c6c31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d54c7be10e874b2b8d0c39a4a44b5253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a981e25ac5d44bd08eab7868585ed1bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a738e84dce684a01a1bacd4d3574ab34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "996a576e712c457db93c99bf69155f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a02c170a70e84f718046b68c77982653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3MFT2x_Qqrf",
        "outputId": "d79a7660-29ac-4e59-ebfb-2a0ff87a6af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.9)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import all the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, roc_auc_score, mean_squared_error\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Fk_TVGuSQ-SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = '/content/drive/My Drive/AMLProject/Data/2024-flame-ai-challenge/dataset'"
      ],
      "metadata": {
        "id": "7KdECFXfQ-O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hovlw4-kQ-MT",
        "outputId": "3b7cad1f-9692-408f-a475-fb7e930e1e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the idea and part of the code for positional and spatial encoding waas taken from the second best solution\n",
        "# in the kaggle competition - https://www.kaggle.com/competitions/2024-flame-ai-challenge/discussion/541458\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=150):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_seq_length, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0)]"
      ],
      "metadata": {
        "id": "BibeazyLH-B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialEncoding(nn.Module):\n",
        "    def __init__(self, d_model, height=113, width=32):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(d_model, height, width)\n",
        "        for pos_h in range(height):\n",
        "            for pos_w in range(width):\n",
        "                for i in range(0, d_model, 2):\n",
        "                    pe[i, pos_h, pos_w] = math.sin(pos_h / (10000 ** (i / d_model)))\n",
        "                    pe[i + 1, pos_h, pos_w] = math.cos(pos_w / (10000 ** ((i + 1) / d_model)))\n",
        "        self.register_buffer('pe', pe)\n",
        "            # Initialize loss tracking\n",
        "        self.epoch_train_losses = []\n",
        "        self.epoch_val_losses = []\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe"
      ],
      "metadata": {
        "id": "oNJXqQA0IAG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredFormer(pl.LightningModule):\n",
        "    def __init__(self, input_dim=7, d_model=32, nhead=8, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input embedding\n",
        "        self.input_proj = nn.Conv2d(input_dim, d_model, 3, padding=1)\n",
        "        self.velocity_proj = nn.Conv2d(d_model, 2, 1)\n",
        "\n",
        "        # Positional encodings\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        self.spatial_encoder = SpatialEncoding(d_model)\n",
        "\n",
        "        # Transformer layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=1024,\n",
        "            batch_first=True,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        # Output projection\n",
        "        self.output_proj = nn.Conv2d(d_model, 1, 3, padding=1)\n",
        "        self.residual_conv = nn.Conv2d(input_dim, 1, 1)\n",
        "\n",
        "        # Metrics tracking\n",
        "        self.train_metrics = []\n",
        "        self.val_metrics = []\n",
        "\n",
        "    def create_flow_grid(self, flow):\n",
        "        b, _, h, w = flow.size()\n",
        "        device = flow.device\n",
        "        grid_x = torch.linspace(-1, 1, w, device=device)\n",
        "        grid_y = torch.linspace(-1, 1, h, device=device)\n",
        "        grid_y, grid_x = torch.meshgrid(grid_y, grid_x)\n",
        "        grid = torch.stack([grid_x, grid_y], dim=-1)\n",
        "        grid = grid.unsqueeze(0).expand(b, -1, -1, -1)\n",
        "        flow = flow.permute(0, 2, 3, 1)\n",
        "        grid = grid + flow\n",
        "        return torch.clamp(grid, -1, 1)\n",
        "\n",
        "    def forward(self, x, future_steps):\n",
        "        b, t, c, h, w = x.size()\n",
        "        x_proj = x.view(b * t, c, h, w)\n",
        "        features = self.input_proj(x_proj)\n",
        "        velocity = self.velocity_proj(features).view(b, t, 2, h, w)\n",
        "\n",
        "        features = features.view(t, b, self.d_model, h, w)\n",
        "        features = self.spatial_encoder(features)\n",
        "        features = features.permute(0, 1, 3, 4, 2).reshape(t, b * h * w, self.d_model)\n",
        "        features = self.pos_encoder(features)\n",
        "\n",
        "        memory = self.transformer(features)\n",
        "        outputs = []\n",
        "        current_state = x[:, -1, -1]\n",
        "        current_velocity = velocity[:, -1]\n",
        "\n",
        "        for _ in range(future_steps):\n",
        "            feature_pred = memory[-1:].reshape(1, b, h, w, self.d_model)\n",
        "            feature_pred = feature_pred.permute(0, 1, 4, 2, 3)\n",
        "            out = self.output_proj(feature_pred.reshape(-1, self.d_model, h, w))\n",
        "            residual = self.residual_conv(x[:, -1])\n",
        "\n",
        "            grid = self.create_flow_grid(current_velocity)\n",
        "            moved_state = F.grid_sample(\n",
        "                current_state.unsqueeze(1),\n",
        "                grid,\n",
        "                mode='bilinear',\n",
        "                padding_mode='zeros',\n",
        "                align_corners=True\n",
        "            )\n",
        "\n",
        "            final_pred = out + residual + moved_state\n",
        "            outputs.append(final_pred)\n",
        "            current_state = final_pred.squeeze(1)\n",
        "            memory = torch.cat([memory[1:], memory[-1:]], 0)\n",
        "\n",
        "        return torch.stack(outputs, dim=1).squeeze(2)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      y_hat = self(x, future_steps=y.size(1))\n",
        "\n",
        "      # Calculate losses\n",
        "      mse_loss = F.mse_loss(y_hat, y)\n",
        "      movement_loss = F.mse_loss(y_hat[:, 1:] - y_hat[:, :-1], y[:, 1:] - y[:, :-1])\n",
        "      loss = mse_loss + 0.5 * movement_loss\n",
        "\n",
        "     # Calculate metrics - detach tensors before converting to numpy\n",
        "      y_flat = y.reshape(-1).cpu().detach()\n",
        "      y_hat_flat = y_hat.reshape(-1).cpu().detach()\n",
        "      f1 = f1_score((y_flat > 0.5).numpy(), (y_hat_flat > 0.5).numpy(), average='binary')\n",
        "      roc_auc = roc_auc_score((y_flat > 0.5).numpy(), y_hat_flat.numpy())\n",
        "\n",
        "      self.log_dict({\n",
        "          'train_loss': loss,\n",
        "          'train_f1': f1,\n",
        "          'train_roc_auc': roc_auc\n",
        "      })\n",
        "      return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      y_hat = self(x, future_steps=y.size(1))\n",
        "\n",
        "      # Calculate losses\n",
        "      mse_loss = F.mse_loss(y_hat, y)\n",
        "      movement_loss = F.mse_loss(y_hat[:, 1:] - y_hat[:, :-1], y[:, 1:] - y[:, :-1])\n",
        "      loss = mse_loss + 0.5 * movement_loss\n",
        "\n",
        "      # Calculate metrics - detach tensors before converting to numpy\n",
        "      y_flat = y.reshape(-1).cpu().detach()\n",
        "      y_hat_flat = y_hat.reshape(-1).cpu().detach()\n",
        "      f1 = f1_score((y_flat > 0.5).numpy(), (y_hat_flat > 0.5).numpy(), average='binary')\n",
        "      roc_auc = roc_auc_score((y_flat > 0.5).numpy(), y_hat_flat.numpy())\n",
        "\n",
        "      self.log_dict({\n",
        "          'val_loss': loss,\n",
        "          'val_f1': f1,\n",
        "          'val_roc_auc': roc_auc\n",
        "      })\n",
        "      return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.001)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": scheduler,\n",
        "            \"monitor\": \"val_loss\"\n",
        "        }"
      ],
      "metadata": {
        "id": "1MHOEhirRHC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WildfireDataset(Dataset):\n",
        "    def __init__(self, csv_file, data_dir, sequence_length=5, prediction_length=20):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.data_dir = data_dir\n",
        "        self.sequence_length = sequence_length\n",
        "        self.prediction_length = prediction_length\n",
        "        self.max_u = self.data['u'].max()\n",
        "        self.max_alpha = self.data['alpha'].max()\n",
        "\n",
        "        # Calculate total number of sequences\n",
        "        self.sequences_per_file = 150 - sequence_length - prediction_length + 1\n",
        "        self.total_sequences = len(self.data) * self.sequences_per_file\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Calculate which file and which sequence within the file\n",
        "        file_idx = idx // self.sequences_per_file\n",
        "        sequence_start = idx % self.sequences_per_file\n",
        "\n",
        "        row = self.data.iloc[file_idx]\n",
        "\n",
        "        # Load data\n",
        "        ustar = np.fromfile(f\"{self.data_dir+'/train'}/{row['ustar_filename']}\", dtype=np.float32).reshape(row['Nt'], row['Nx'], row['Ny'])\n",
        "        theta = np.fromfile(f\"{self.data_dir+'/train'}/{row['theta_filename']}\", dtype=np.float32).reshape(row['Nt'], row['Nx'], row['Ny'])\n",
        "        xi = np.fromfile(f\"{self.data_dir+'/train'}/{row['xi_filename']}\", dtype=np.float32).reshape(row['Nt'], row['Nx'], row['Ny'])\n",
        "\n",
        "        # Create global parameter channels\n",
        "        u_channel = np.full((row['Nt'], 1, row['Nx'], row['Ny']), row['u'], dtype=np.float32)\n",
        "        alpha_channel = np.full((row['Nt'], 1, row['Nx'], row['Ny']), row['alpha'], dtype=np.float32)\n",
        "\n",
        "        # Normalize data\n",
        "        # Add safety check for normalization\n",
        "        if np.std(ustar) == 0:\n",
        "          ustar = ustar - np.mean(ustar)  # Only center if std is zero\n",
        "        else:\n",
        "          ustar = (ustar - np.mean(ustar)) / np.std(ustar)\n",
        "        if np.std(theta) == 0:\n",
        "          theta = theta - np.mean(theta)  # Only center if std is zero\n",
        "        else:\n",
        "          theta = (theta - np.mean(theta)) / np.std(theta)\n",
        "\n",
        "        # Normalize global parameters\n",
        "        u_channel = u_channel / self.max_u\n",
        "        alpha_channel = alpha_channel / self.max_alpha\n",
        "\n",
        "        # Select sequence window\n",
        "        sequence_end = sequence_start + self.sequence_length\n",
        "        target_start = sequence_end\n",
        "        target_end = target_start + self.prediction_length\n",
        "\n",
        "        # Create input sequence\n",
        "        ustar_seq = ustar[sequence_start:sequence_end]\n",
        "        theta_seq = theta[sequence_start:sequence_end]\n",
        "        xi_seq = xi[sequence_start:sequence_end]\n",
        "        u_seq = u_channel[sequence_start:sequence_end]\n",
        "        alpha_seq = alpha_channel[sequence_start:sequence_end]\n",
        "\n",
        "        # Stack input channels\n",
        "        input_seq = np.stack([ustar_seq, theta_seq, xi_seq], axis=1)\n",
        "\n",
        "        # Add positional encoding\n",
        "        pos_enc = self.positional_encoding(self.sequence_length, row['Nx'], row['Ny'])\n",
        "\n",
        "        # Concatenate all channels\n",
        "        input_seq = np.concatenate([input_seq, pos_enc, u_seq, alpha_seq], axis=1)\n",
        "\n",
        "        # Create target sequence\n",
        "        if target_end <= 150:\n",
        "            target = xi[target_start:target_end]\n",
        "        else:\n",
        "            # Pad with zeros if target extends beyond available timesteps\n",
        "            available_steps = 150 - target_start\n",
        "            target = np.zeros((self.prediction_length, row['Nx'], row['Ny']), dtype=np.float32)\n",
        "            if available_steps > 0:\n",
        "                target[:available_steps] = xi[target_start:150]\n",
        "\n",
        "        return torch.FloatTensor(input_seq), torch.FloatTensor(target)\n",
        "\n",
        "    def positional_encoding(self, seq_len, height, width):\n",
        "        pos_enc = np.zeros((seq_len, 2, height, width), dtype=np.float32)\n",
        "        for t in range(seq_len):\n",
        "            for i in range(height):\n",
        "                for j in range(width):\n",
        "                    pos_enc[t, 0, i, j] = i / height\n",
        "                    pos_enc[t, 1, i, j] = j / width\n",
        "        return pos_enc"
      ],
      "metadata": {
        "id": "fx9Fz6Pfp879"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY_3vSy0qAj_",
        "outputId": "ab229fb5-d8a4-4346-e0b4-62e2500a5bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(os.path.join(input_path, 'train.csv'))"
      ],
      "metadata": {
        "id": "Ktqj4R1lqCzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Initialize dataset and create data loaders\n",
        "train_dataset = WildfireDataset(\n",
        "    csv_file=os.path.join(input_path, 'train.csv'),\n",
        "    data_dir=input_path,\n",
        "    sequence_length=5,\n",
        "    prediction_length=20\n",
        ")\n",
        "\n",
        "# 8. Create train-validation split and data loaders\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,  # Increase from 4\n",
        "    pin_memory=True  # Enable for GPU training\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,  # Set to False for validation\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "Z-uhth0LqFfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()  # Clear GPU memory before training\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# Add to environment variables\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Clear cache before training\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7ionDnnd1CI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PredFormer(\n",
        "    input_dim=7,      # Keep input channels\n",
        "    d_model=32,\n",
        "    nhead=4,\n",
        "    num_layers=2\n",
        ")\n",
        "#model = torch.compile(model)\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=os.path.join(output_path, 'checkpoints'),\n",
        "    filename='wildfire-{epoch:02d}-{val_loss:.2f}',\n",
        "    save_top_k=3,\n",
        "    monitor='val_loss',\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "torch.cuda.empty_cache()  # Clear GPU memory\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=20,\n",
        "    accelerator='cpu',  # Change from 'gpu' to 'cpu'\n",
        "    callbacks=[checkpoint_callback],\n",
        "    default_root_dir=output_path,\n",
        "    gradient_clip_val=0.5,\n",
        "    accumulate_grad_batches=4\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "def plot_training_history(model):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(model.epoch_train_losses, label='Training Loss')\n",
        "    plt.plot(model.epoch_val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# 10. Start training\n",
        "plot_training_history(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434,
          "referenced_widgets": [
            "661b16a636d44d2eb0b2b47526e3fe8a",
            "f3c1d065598042279a9d69405306395e",
            "9e43440db1d1424897cf2b9d19bb2b2a",
            "1d73bc4266884106b6b1de90da88c020",
            "84dc00d1172b4b86a09f5132c3fd309e",
            "91903ffbce244a86b803464d154c6c31",
            "d54c7be10e874b2b8d0c39a4a44b5253",
            "a981e25ac5d44bd08eab7868585ed1bb",
            "a738e84dce684a01a1bacd4d3574ab34",
            "996a576e712c457db93c99bf69155f48",
            "a02c170a70e84f718046b68c77982653"
          ]
        },
        "id": "fEfZutyJRG_Z",
        "outputId": "d427a9a0-4ebc-4b0d-e71d-03358414d874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /content/drive/My Drive/AMLProject/Data/2024-flame-ai-challenge/checkpoints exists and is not empty.\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name            | Type               | Params | Mode \n",
            "---------------------------------------------------------------\n",
            "0 | input_proj      | Conv2d             | 2.0 K  | train\n",
            "1 | velocity_proj   | Conv2d             | 66     | train\n",
            "2 | pos_encoder     | PositionalEncoding | 0      | train\n",
            "3 | spatial_encoder | SpatialEncoding    | 0      | train\n",
            "4 | transformer     | TransformerEncoder | 141 K  | train\n",
            "5 | output_proj     | Conv2d             | 289    | train\n",
            "6 | residual_conv   | Conv2d             | 8      | train\n",
            "---------------------------------------------------------------\n",
            "144 K     Trainable params\n",
            "0         Non-trainable params\n",
            "144 K     Total params\n",
            "0.577     Total estimated model params size (MB)\n",
            "28        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "661b16a636d44d2eb0b2b47526e3fe8a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "def create_and_save_fire_animation(model, test_loader, filename=None):\n",
        "    x_test, y_test = next(iter(test_loader))\n",
        "    x = x_test[0:1]\n",
        "    y_true = y_test[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(x, future_steps=y_true.size(0))\n",
        "        y_pred = y_pred[0]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Create imshow objects once\n",
        "    im1 = ax1.imshow(y_true[0].cpu(), cmap='hot', vmin=0, vmax=1)\n",
        "    im2 = ax2.imshow(y_pred[0].cpu(), cmap='hot', vmin=0, vmax=1)\n",
        "\n",
        "    # Create colorbars once\n",
        "    fig.colorbar(im1, ax=ax1)\n",
        "    fig.colorbar(im2, ax=ax2)\n",
        "\n",
        "    def update(frame):\n",
        "        im1.set_array(y_true[frame].cpu())\n",
        "        im2.set_array(y_pred[frame].cpu())\n",
        "        ax1.set_title(f'Actual Fire (t={frame})')\n",
        "        ax2.set_title(f'Predicted Fire (t={frame})')\n",
        "        return im1, im2\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, update, frames=y_true.size(0),\n",
        "        interval=200, blit=True\n",
        "    )\n",
        "\n",
        "    if filename:\n",
        "        if filename.endswith('.mp4'):\n",
        "            Writer = animation.writers['ffmpeg']\n",
        "            writer = Writer(fps=5, metadata=dict(artist='Me'), bitrate=1800)\n",
        "            anim.save(filename, writer=writer)\n",
        "            files.download(filename)\n",
        "        elif filename.endswith('.gif'):\n",
        "            anim.save(filename, writer='pillow')\n",
        "            files.download(filename)\n",
        "\n",
        "    plt.close()\n",
        "    return anim\n",
        "\n",
        "# Create and save animations\n",
        "anim_gif = create_and_save_fire_animation(model, val_loader, 'fire_prediction.gif')\n",
        "\n",
        "# Display in notebook\n",
        "HTML(anim_gif.to_jshtml())"
      ],
      "metadata": {
        "id": "gDW6c2SvRG9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGbGVCU_RG6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "viWM19RbRG31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mi180tXRG1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gz5-DDgRRGys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4L5yKT1RGwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KG8seJoLRGtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCFilTNxQ-Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCsTh0s5Q-HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HV5eZ0sNQ-Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FfmmCKkQ-CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQKVa7uQQ9_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTo_yn5GQ99e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9qwMuKcQ96_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qk4TFhOSQ94k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0XzwonZwQ92F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jFo56cESQ9zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEKtviPIQ9xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RgntxLc1Q9uq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}